```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/bwright/Documents/R")
library(glmnet)
library(ISLR)
library(plotmo)
library(gradDescent)
library(caret)
library(dplyr)
```

We are going to use white wine quality to walk through a example of gradient regression and ridge regression. Our goal is to develop a model to predict the quality of white wine. 
```{r}
getwd()

white <- read.csv('winequality-white.csv')
head(white)
str(white)
View(white)
#what's wrong with using quality as a target vector?

cor(white)

#So let's start with a normal regression model. 
white_stand <- data.frame(scale(white)) #We are going to need to standardize our inputs for penalized regression so let's go ahead and do that work the OLS regression for ease of comparison. 

ols_white <- lm(alcohol~., data = white_stand)
summary(ols_white) #Ok so what does this tell us? 

ols_white_coef <- data.frame(ols_white$coefficients)
View(ols_white_coef)
 
ols_white_pred <- predict(ols_white, white_stand)
head(ols_white_pred)
head(white_stand$alcohol)
```

Ok now let's do regression with a gradiant 
```{r}
#We should really split the data into test and train at this point, we are going to use the caret package but I'll show a manual way later as well

#Here we are creating a index to use to split the data
gd_trainIndex <- createDataPartition(white_stand$alcohol, p=.66, list=FALSE,times = 1)

#Here we are using the index
gd_white_tr <- white_stand[gd_trainIndex, ]
gd_white_te <- white_stand[-gd_trainIndex, ]
dim(gd_white_tr)
dim(gd_white_te)

gd_white_tr <- gd_white_tr%>%select(-alcohol,everything())
str(gd_white_tr)
gd_white_te <- gd_white_te%>%select(-alcohol,everything())
str(gd_white_te)

GD_White <- gradDescentR.learn(gd_white_tr,learningMethod = "GD", control = list(.001,100), seed = 100)

GD_White$model
pred_GD_White <- predict(GD_White, gd_white_te)
head(pred_GD_White)
pred_GD_whitev1 <- pred_GD_White$V1
head(pred_GD_whitev1)


```



```{r}
#First let's split our data in train and test, using say 2/3 train, 1/3 test and target/predictor variables

n_obs = dim(white_stand)[1]
n_obs
prop_split = 0.66
train_index = sample(1:n_obs, round(n_obs * prop_split))
predictors <- white_stand[c(1:10,12)]
head(predictors)
target <- white_stand$alcohol
head(target)
#Ridge Regression Requires a Matrix vice a Dataframe
predictors <- model.matrix(fixed.acidity~., predictors)
str(predictors)
pred_tr = predictors[train_index,]
pred_te = predictors[-train_index,]
target_tr = target[train_index]
target_te = target[-train_index]

library(glmnet)
#Ok let's build the model and see what happens
ridge.white <- glmnet(pred_te,target_te, alpha = 0, nlambda = 100, lambda.min.ratio = .0001)


#Let's look with the least amount of penality 
white_coefs100 <- data.frame(coef(ridge.white)[,100])
View(white_coefs100)

#Now let's look at the highest penality 
white_coefs1<- data.frame(coef(ridge.white)[,1])
View(white_coefs1)

white_lambda <- data.frame(ridge.white$lambda)
View(white_lambda)

#We can also take a look at a more fine tuned graphical output 


plot_glmnet(ridge.white,xvar="lambda",label=5)
plot_glmnet(ridge.white,label=5)


#Ok but this doesn't use cross validation to train the penality, so let's see an example using cv.glmnet 
set.seed(100)
cv.white.ridge <- cv.glmnet(predictors, target, alpha=0, nlambda=100, lambda.min.ratio=.0001)
#Let's take a look and see if we can see the minumum
plot(cv.white.ridge)

#Let's take a look under the hood, here we have the lambda outputs, the mean cross validated error and the estimate of the standard error of the cvm
cv.white.ridge.output <- data.frame(cv.white.ridge$lambda,cv.white.ridge$cvm,cv.white.ridge$cvsd)

View(cv.white.ridge.output)

#This will provide our best lambda output
best.penality <- cv.white.ridge$lambda.min
best.penality


white.best.coef <- data.frame(predict(ridge.white, s=best.penality, type="coefficients")[1:12, ])
View(white.best.coef)
cv.white.ridge$lambda


#Finally let's compare our models using RMSE
#predict with OLS, GD and Ridge and see what happens  

y_hat_ols <- predict(ols_white, white_stand)
RMSE_OLS <- sqrt(mean((white$alcohol-y_hat_ols)^2))
RMSE_OLS


y_hat_gd <- pred_GD_whitev1
RSME_gd <- sqrt(mean((gd_white_te-y_hat_gd)^2))
RSME_gd

y_hat_ridge <- predict(cv.white.ridge, pred_te)
RMSE_Ridge <- sqrt(mean((target_te-y_hat_ridge)^2))
RMSE_Ridge 

y_hat_lasso <- predict(cv.white.lasso, pred_te)
RMSE_Lasso <- sqrt(mean((target_te-y_hat_lasso)^2)) 
RMSE_Lasso

#Also a RSME function called...RSME

```

Contours
```{r}
#But first let's look at contours real quick 

library(plotly)
str(volcano)
dim(volcano)
contour_example <- plot_ly(z=~volcano, type="contour") 
contour_example
```


Ok no let's do it as a classifier 
```{r}
library(forcats)
library(car)
table(white$quality)
quantile(as.numeric(white$quality))
boxplot(as.numeric(white$quality))
bi_quality <- cut(as.numeric(white$quality), breaks = c(0,6,9),labels = c(0,1))
head(bi_quality)
table(bi_quality)
target_bi <- bi_quality

str(white)

pred_bi <- as.matrix(white)[,-12]
head(pred_bi)

bi_ridge_wq <- glmnet(pred_bi, y=as.factor(target_bi), alpha=0, family="binomial")
plot(bi_ridge_wq, xvar="lambda")
bi_ridge_wq
coef(bi_ridge_wq)[, 1]
coef(bi_ridge_wq)[, 100]
#We can also use the cross validated approach 
bi_ridge_wq_cv <- cv.glmnet(pred_bi, y=as.factor(target_bi), alpha=0, family="binomial")
plot(bi_ridge_wq_cv)
best.lambda <- bi_ridge_wq_cv$lambda.1se
best.lambda


#Ok now let's do some predicting and use the 50% threshold for our value labelling
pred.model1 <- predict(bi_ridge_wq_cv,pred_bi,type='response')
#iselse works like so...ifelse(test, yes,no), so we are classifying the output if greater than .05 label as 1 if not lable as a 0
head(pred.model1)
pred.model1.1 <- ifelse(pred.model1 > 0.5,1,0)
#essentially we are creating percentage likelihood of default for each value, above 50% we are saying it's more likely to occur. 
head(pred.model1.1)
table(pred.model1.1)
model1hit <- mean(pred.model1.1==target_bi)
model1hit



```


Ok so let's add a lasso example
```{r}
#Just a repeat of what we had above.....
n_obs = dim(white_stand)[1]
n_obs
prop_split = 0.66
train_index = sample(1:n_obs, round(n_obs * prop_split))
predictors <- white_stand[c(1:10,12)]#Here we are omitting the target variable
head(predictors)
target <- white_stand$alcohol#Here we are creating the target variable 
head(target)

#Lasso also Regression Requires a Matrix vice a Dataframe, just like ridge
predictors <- model.matrix(fixed.acidity~., predictors)
str(predictors)
head(predictors)
pred_tr = predictors[train_index,]
pred_te = predictors[-train_index,]
target_tr = target[train_index]
target_te = target[-train_index]
set.seed(101)
#here we are going to train our lambda then embed it into our lasso model 
cv.white.lasso <- cv.glmnet(pred_tr, target_tr, family="gaussian", alpha=1, nlambda=100, lambda.min.ratio=.0001)

coef(cv.white.lasso, s=cv.white.lasso$lambda.1se)

#if we embed it back into another lasso we get the same result
cv.white.lasso.1 <- glmnet(pred_tr, target_tr, alpha=1,lambda = cv.white.lasso$lambda.1se)
#same coefs

coef(cv.white.lasso.1)

plot(cv.white.lasso, xvar = 'lambda')
```
Ok now take the red-wine dataset and run lasso, but also adjust the hyper-parameters including the k-fold number and lambda ratio (alpha) and compare results. 
